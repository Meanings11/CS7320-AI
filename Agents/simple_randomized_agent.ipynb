{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents: Vacuum-cleaner World\n",
    "\n",
    "Implement a simulator environment for a vacuum-cleaner world and a set of intelligent agents.\n",
    "\n",
    "## PEAS description\n",
    "\n",
    "__Performance Measure:__ Each action costs 1. The performance is measured as the sum of the cost to clean the whole environment.\n",
    "\n",
    "__Environment:__ A room with $n \\times n$ squares where $n = 5$. Dirt is randomly placed on each square with probability $p = 0.2$. For simplicity, you can assume that the agent knows the size of the layout of the room (i.e., it knows n and where it starts).\n",
    "\n",
    "__Actuators:__ The agent can `clean` the current square or move to an adjacent square by going `north`, `east`, `west`, or `south`.\n",
    "\n",
    "__Sensors:__ Four bumper sensors, one for north`, `east`, `west`, and `south`; a dirt sensor reporting dirt in the current square.  \n",
    "\n",
    "The easiest implementation for the environment is to hold an 2-dimensional array to represent if squares are clean or dirty and to call the agent function in a loop untill all squares are clean or a predefined number of steps have been reached.\n",
    "\n",
    "## Define the agent program for a simple randomized agent\n",
    "\n",
    "The agent program is a function that gets sensor information (the current percepts) as the arguments. The arguments are:\n",
    "\n",
    "* A dictonary with boolean entries for the for bumper sensors `north`, `east`, `west`, `south`; not specified bumpers are assumed to be `False`. E.g., if the agent is on the north-west corner, `bumpers` gets `{\"north\" : True, \"west\" : True}` or if the agent is not close to a border then it gets `{}`.\n",
    "* The dirt sensor produces a boolean.\n",
    "\n",
    "The agent returns the chosen action as a string.\n",
    "\n",
    "Here is an example implementation for the agent program of a simple randomized agent:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "actions = [\"north\", \"east\", \"west\", \"south\", \"suck\"]\n",
    "\n",
    "\n",
    "def simple_randomized_agent(bumpers, dirty):\n",
    "    return random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'south'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_randomized_agent({\"south\" : True}, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple environment example\n",
    "\n",
    "The environment is infinite in size (bumpers are always `False`) and every square is dirty. We run the agent for 10 times steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suck\n",
      "south\n",
      "west\n",
      "suck\n",
      "suck\n",
      "south\n",
      "north\n",
      "north\n",
      "south\n",
      "west\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(simple_randomized_agent({\"north\" : False, \"south\" : False, \"west\" : False, \"east\" : False}, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "_Submission Instructions:_ Use this notebook to prepare your submission. Complete this section with your code and results. You can use Markdown blocks for your description, comments in the code and use mathplotlib to produce charts. If you use external code files then you can include them with \n",
    "\n",
    "```\n",
    "from notebook import psource\n",
    "psource(\"your_file.py\")\n",
    "```\n",
    "\n",
    "_Note:_ Try to keep the code simple! In this couse, we want to learn about the algorithms and we often do not need to use object-oriented design. \n",
    "\n",
    "\n",
    "## Task 1: Implement a simulation environment\n",
    "\n",
    "Your environment simulator needs to create squares, make some dirty, and proivde the agent function with the sensor inputs. The environment needs to evaluate the performance measure. It needs to track the agent's actions until all dirty squares are clean and count the number of actions it takes the agent to complete the task.\n",
    "\n",
    "The simulation environment needs to work with the simple randomized agent program from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Room Size 25\n",
      "Dirty squares: 9\n",
      "[[False False False  True  True]\n",
      " [False False False False False]\n",
      " [ True  True False  True False]\n",
      " [ True  True False False  True]\n",
      " [False False False  True False]]\n",
      "Step  1\n",
      "Bumpers:  {'north': True, 'south': False, 'west': True, 'east': False}\n",
      "Action:  north\n",
      "Current Position:  [0, 0]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  2\n",
      "Bumpers:  {'north': True, 'south': False, 'west': True, 'east': False}\n",
      "Action:  west\n",
      "Current Position:  [0, 0]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  3\n",
      "Bumpers:  {'north': True, 'south': False, 'west': True, 'east': False}\n",
      "Action:  east\n",
      "Current Position:  [0, 1]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  4\n",
      "Bumpers:  {'north': True, 'south': False, 'west': False, 'east': False}\n",
      "Action:  east\n",
      "Current Position:  [0, 2]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  5\n",
      "Bumpers:  {'north': True, 'south': False, 'west': False, 'east': False}\n",
      "Action:  east\n",
      "Current Position:  [0, 3]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  6\n",
      "Bumpers:  {'north': True, 'south': False, 'west': False, 'east': False}\n",
      "Action:  west\n",
      "Current Position:  [0, 2]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  7\n",
      "Bumpers:  {'north': True, 'south': False, 'west': False, 'east': False}\n",
      "Action:  south\n",
      "Current Position:  [1, 2]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  8\n",
      "Bumpers:  {'north': False, 'south': False, 'west': False, 'east': False}\n",
      "Action:  east\n",
      "Current Position:  [1, 3]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  9\n",
      "Bumpers:  {'north': False, 'south': False, 'west': False, 'east': False}\n",
      "Action:  south\n",
      "Current Position:  [2, 3]\n",
      "Dirty squares:  9 \n",
      "\n",
      "Step  10\n",
      "Bumpers:  {'north': False, 'south': False, 'west': False, 'east': False}\n",
      "Action:  east\n",
      "Current Position:  [2, 4]\n",
      "Dirty squares:  9 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def getBumpter(n, pos):\n",
    "    return {\"north\": pos[0] == 0,\n",
    "            \"south\": pos[0] == n-1,\n",
    "            \"west\":  pos[1] == 0,\n",
    "            \"east\":  pos[1] == n-1}\n",
    "\n",
    "def environment(agent, times, n=5, debug=False):\n",
    "    room = [[np.random.choice([True, False], p = [0.2, 0.8]) for i in range(n)] for j in range(n)]\n",
    "    dirtyNum = np.sum(room)\n",
    "    print(\"Room Size\", n*n)\n",
    "    print(\"Dirty squares:\", dirtyNum)\n",
    "    print(np.matrix(room))\n",
    "    \n",
    "    pos = [0,0]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(times):\n",
    "        count = count + 1\n",
    "        if debug: print(\"Step \", count)\n",
    "        bumpers = getBumpter(n,pos)\n",
    "        if debug: print(\"Bumpers: \", bumpers)\n",
    "        action = agent(bumpers, room[pos[0]][pos[1]])\n",
    "        if debug:print(\"Action: \", action)\n",
    "\n",
    "        if action == \"suck\": room[pos[0]][pos[1]] = False\n",
    "        if action == \"north\" and (pos[0] > 0): pos[0] = pos[0]-1\n",
    "        if action == \"south\" and (pos[0] < n-1): pos[0] = pos[0]+1\n",
    "        if action == \"west\" and (pos[1] > 0): pos[1] = pos[1]-1\n",
    "        if action == \"east\" and (pos[1] < n-1): pos[1] = pos[1]+1\n",
    "        if debug: print(\"Current Position: \", pos);\n",
    "        \n",
    "        dirtyNum = np.sum(room)\n",
    "        if debug: print(\"Dirty squares: \", dirtyNum, \"\\n\")\n",
    "        if dirtyNum <= 0: break\n",
    "        \n",
    "\n",
    "    return count\n",
    "\n",
    "        \n",
    "environment(simple_randomized_agent,5,debug = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:  Implement a simple reflex agent\n",
    "\n",
    "The simple reflex agent randomly walks around but reacts to the bumper sensor by not bumping into the wall and to dirt with sucking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Room Size 25\n",
      "Dirty squares: 8\n",
      "[[ True  True False False  True]\n",
      " [False False False False False]\n",
      " [ True  True False False False]\n",
      " [False False False False False]\n",
      " [ True  True False False  True]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_reflex_agent(bumpers, dirty):\n",
    "    actions = [\"north\", \"east\", \"west\", \"south\"]\n",
    "    if dirty: return \"suck\"\n",
    "    \n",
    "    # remove directions where bump sensors are triggered\n",
    "    for i in bumpers:\n",
    "        if bumpers[i]: actions.remove(i)\n",
    "    \n",
    "    return random.choice(actions)\n",
    "\n",
    "environment(simple_reflex_agent, 1000, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement a model-based reflex agent \n",
    "\n",
    "This agent keeps track of the location and remembers where it has cleaned. Assume the agent knows how many squares the room has and where it starts. It can now use more advanced navigation.\n",
    "\n",
    "_Note on implementing the state:_ You can use a global variable. In Python, you have to use the keyword `global` in your function for this to work (see: https://www.programiz.com/python-programming/global-keyword). Alternatively, you can define a class for your agent with a member variable for the state and a function for the agent program (see: https://www.w3schools.com/python/python_classes.asp). \n",
    "\n",
    "Describe how your agent will perform \n",
    "* if it is put into a larger room, \n",
    "* if the room contains obstacles, or \n",
    "* if it starts in a random square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [0, 1], [1, 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'south'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = []\n",
    "\n",
    "def model_based_reflex_agent(bumpers, dirty, pos):\n",
    "    #add new pos to the state\n",
    "    global state \n",
    "    state.append(pos)\n",
    "    \n",
    "    actions = [\"north\", \"east\", \"west\", \"south\"]\n",
    "    if dirty: return \"suck\"\n",
    "    \n",
    "    # remove directions where bump sensors are triggered\n",
    "    for i in bumpers:\n",
    "        if bumpers[i]: actions.remove(i)\n",
    "    \n",
    "    print(state)\n",
    "    actionList = actions.copy()\n",
    "    \n",
    "    for a in actions:\n",
    "        if a == \"north\" and ([pos[0]-1,pos[1]] in state): actionList.remove(a)\n",
    "        if a == \"south\" and ([pos[0]+1,pos[1]] in state): actionList.remove(a)\n",
    "        if a == \"west\" and ([pos[0],pos[1]-1] in state): actionList.remove(a)\n",
    "        if a == \"east\" and ([pos[0],pos[1]+1] in state): actionList.remove(a)\n",
    "    \n",
    "#     return actionList\n",
    "    return random.choice(actionList)\n",
    "\n",
    "# bumper =  {'north': False, 'south': False, 'west': False, 'east': False}\n",
    "# state.append([1,0])\n",
    "# state.append([0,1])\n",
    "# model_based_reflex_agent(bumper, False, [1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Simulation study\n",
    "\n",
    "Compare the performance of the agents using different size environments. E.g., $5 \\times 5$, $10 \\times 10$ and\n",
    "$100 \\times 100$. Use at least 100 random runs for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus tasks\n",
    "\n",
    "* __Obstacles:__ Add random obstacle squares that also trigger the bumper sensor. The agent does not know where the obstacles are. Observe how this changes the performance of the three implementations.\n",
    "* __Unknown environment with obstacles:__ Implement an agent for an environment where the agent does not know how large the environment is (we assume it is rectangular), where it starts or where the obstacles are. An option would be to always move to the closest unchecked/uncleaned square.\n",
    "* __Utility-based agent:__ Change the environment, so each square has a fixed probability of getting dirty again. We assume the agent has learned this information over time. For the implementation, we give this information to the agent as a 2-dimensional array of probabilities  Cleaning one dirty square produces a utility of 1. Implement a utility-based agent that maximizes the expected utility over one full charge which lasts for 10000 time steps. This is very tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
